{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de7f303",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26197bc6",
   "metadata": {},
   "source": [
    "# Data Ingestion for Reporting Optimization\n",
    "\n",
    "...or how to convert files into tables of a database in just __5 steps!!!__\n",
    "\n",
    "![Image](./images/data_ingestion_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28621ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Import the libraries (only 3 libraries!!!)\n",
    "\n",
    "__[DuckDB](https://duckdb.org/)__ is a fast in-process analytical database. DuckDB supports a feature-rich SQL dialect complemented with deep integrations into client APIs. It provides a full-featured SQL engine designed for analytics and __OLAP (Online Analytical Processing)__ tasks. It can run SQL queries on large datasets efficiently.\n",
    "\n",
    "__Tip:__ _Don't forget to install DuckDB._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import duckdb     # pip install duckdb\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d938d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: List your `.csv` data files and create the relative paths list\n",
    "\n",
    "Using the `os` library we can list the filenames we want load as tables into our database. \n",
    "\n",
    "__Tip:__ _Remember to be clean and neat when naming the `.csv` files._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List .csv filenames\n",
    "\n",
    "dir_list = os.listdir('./datasets/modeling/')\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relative paths list (i.e.: tables)\n",
    "\n",
    "tables = [f'./datasets/modeling/{file}' for file in dir_list]\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b218290",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Create the DDL queries\n",
    "\n",
    "In SQL we have __Data Definition Language__ to define, alter, or manage the structure of database objects like tables, indexes, views, and schemas.\n",
    "\n",
    "__Tip:__ _The code is set to work with an specific path length (i.e.: `table[20:-4]`). Bear in mind that you might need to set it for your use case._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27903e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queries\n",
    "\n",
    "queries = [f\"CREATE OR REPLACE TABLE {table[20:-4]} AS SELECT * FROM '{table}';\" for table in tables]\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545be6c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Load your tables into your Database\n",
    "\n",
    "__DuckDB__ is embedded into applications, meaning it doesn't require a separate server to run. It operates within the same process as the application using it. It can work with in-memory data for fast processing, but it also supports persistent storage for saving data to disk. In our case we're going to use the option of persisting the data in `.db` files.\n",
    "\n",
    "__Tip:__ _Be careful where you store your `.db` files; don't lose them._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection and .db file\n",
    "\n",
    "con = duckdb.connect('./datasets/sales_database.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80019db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables in database and load data from .csv files\n",
    "\n",
    "for query in queries:\n",
    "    con.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bde900",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Check your tables\n",
    "\n",
    "The __DESCRIBE__ SQL command, also known as EXPLAIN in some SQL contexts, retrieves the structure of a specific table. It returns details like column names, data types, and constraints.\n",
    "\n",
    "__Tip:__ _Remember to check every table of your database. Double checking is always a good practice...and don't forget to close the connection_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result for a specific table.\n",
    "\n",
    "con.sql(\"DESCRIBE Sales_short;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection explicitly\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2683b6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Now we're ready to rumble!!!\n",
    "\n",
    "![Image](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExdGcwbHlpMmVvaTdrYmdsYmhqYzZ3YzNqd3o5bGxsNDgzY3I3MW43cCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZJPSFNLmADueHvzoZ8/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ironhack]",
   "language": "python",
   "name": "conda-env-.conda-ironhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
